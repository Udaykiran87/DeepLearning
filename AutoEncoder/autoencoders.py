# -*- coding: utf-8 -*-
"""AutoEncoders.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KYKvdVQ_r32eT7o-_QqOcD_XtCc92F1t

#AutoEncoders

##Downloading the dataset

###ML-100K
"""

"""!wget "http://files.grouplens.org/datasets/movielens/ml-100k.zip"
!unzip ml-100k.zip
!ls"""

"""###ML-1M"""

"""!wget "http://files.grouplens.org/datasets/movielens/ml-1m.zip"
!unzip ml-1m.zip
!ls"""

"""##Importing the libraries"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
from torch.autograd import Variable

"""## Importing the dataset"""

# We won't be using this dataset.
movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')
users = pd.read_csv('ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')
ratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')

"""## Preparing the training set and the test set"""

training_set = pd.read_csv('ml-100k/u1.base', delimiter = '\t')
training_set = np.array(training_set, dtype = 'int')
test_set = pd.read_csv('ml-100k/u1.test', delimiter = '\t')
test_set = np.array(test_set, dtype = 'int')

"""## Getting the number of users and movies"""

nb_users = int(max(max(training_set[:, 0], ), max(test_set[:, 0])))
nb_movies = int(max(max(training_set[:, 1], ), max(test_set[:, 1])))

"""## Converting the data into an array with users in lines and movies in columns"""

def convert(data):
  new_data = []
  for id_users in range(1, nb_users + 1):
    id_movies = data[:, 1] [data[:, 0] == id_users]
    id_ratings = data[:, 2] [data[:, 0] == id_users]
    ratings = np.zeros(nb_movies)
    ratings[id_movies - 1] = id_ratings
    new_data.append(list(ratings))
  return new_data
training_set = convert(training_set)
test_set = convert(test_set)

"""## Converting the data into Torch tensors"""

training_set = torch.FloatTensor(training_set)
test_set = torch.FloatTensor(test_set)

"""## Creating the architecture of the Neural Network"""
class SAE(nn.Module): # Module: Parent class, SAE: Inherited class
    def __init__(self, ): # No extra parameter for __init__, because it is a inherited class and will inherit all elements from its parent class
        super(SAE, self).__init__() # Super: used to inherit all methods and classes from parent class
        self.fc1 = nn.Linear(nb_movies, 20) # fc1: is  a first full connected object of linear class related auto encoder object calles self  --encoding. Numbers are based on tutor's experience.
        self.fc2 = nn.Linear(20, 10) # fc2: is  a second full connected object of linear class related auto encoder object calles self --encoding. Numbers are based on tutor's experience.
        self.fc3 = nn.Linear(10, 20) # fc3: is  a third full connected object of linear class related auto encoder object calles self  --decoding. Numbers are based on tutor's experience.
        self.fc4 = nn.Linear(20, nb_movies) # fc4: is  a fourth full connected object of linear class related auto encoder object calles self --decoding. Numbers are based on tutor's experience.
        self.activation = nn.Sigmoid() # activation: activation function of sigmoid class used in above four full connection layers
        
    def forward(self, x): # action taking place in auto encoders i.e encoders and decoders
        x = self.activation(self.fc1(x)) # initialy, x = input feature vector on the left side of the first fully connected layer,
                                         # new x is the result of the first encoding at first fully connected layer
                                         # which returns the first encoded vector in first hidden layer
                                         # first layer encoding is done here
        x = self.activation(self.fc2(x)) # initialy, x = vector on the left side of the second fully connected layer,
                                         # new x is the result of the second encoding at second fully connected layer
                                         # which returns the second encoded vector in second hidden layer
                                         # second layer encoding is done here
        x = self.activation(self.fc3(x)) # initialy, x = vector on the left side of the third fully connected layer,
                                         # new x is the result of the first decoding at third fully connected layer
                                         # which returns the first decoded vector in third hidden layer
                                         # first layer decoding is done here
        x = self.fc4(x)                  # initialy, x = vector on the left side of the fourth fully connected layer,
                                         # new x is the result of the second decoding at fourth fully connected layer
                                         # which returns the second decoded vector in fourth and final hidden layer
                                         # second layer decoding is done here
                                         # Since this is the last layer, so activation is not applied.
        return x
sae = SAE()  # sae is objct of SAE class.  
criterion = nn.MSELoss() # criterian is used for loss function, which is an object of MSELoss class. It is used to calculate mean squared error.
optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5) # sae.parameters() = automatically invoke all parameters/members inside SAE class.
                                                                           # lr is learning rate based on the tutor's experience.
                                                                           # weight_decay is used reduce the learning rate after every few epochs in order to regulate the convergence. 
                                                                           # This parameter improves model. The value is based on tutor's experience.


"""## Training the SAE"""
nb_epoch = 200  # to define number of epoch, which based on experimentation by trainer
for epoch in range(1, nb_epoch + 1):
    train_loss = 0 # initialize loss error
    s = 0. # float number,  number of users who atleast rated one movie, this variable is required to optimize the memory by not doing any computation for the movies not rated by user.
    for id_user in range(nb_users):
        input = Variable(training_set[id_user]).unsqueeze(0)# unsqueeze(0): is a additional fake dimesion coressponds to one batch as per pytorch syntax
                                                            # this is exactly same as keras syntex used in CNN algorith to create a 1D vector
                                                            # here is instaed of batch learning, it is an online learning.
                                                            # that means weight is going to be updated after each observation or each line instaed of a batch consiting of several lines.
                                                            # target is a clone or exact copy of input. i.e target vector will contain exatly all fetaures of input vector (or ratings of different movies), 
                                                            
        target = input.clone()                              # but then in this case we will modify the target
                                                            # to optimize or save as much memory as possible as this if con will look for those users who atleast rated one movie
                                                            # and if an observation contains only zeros, which means that the user didn't rate any movies, so we use target.data > 0, means all ratings which are greater than zero
        if torch.sum(target.data > 0) > 0:                  # sum > 0, means the observation contains at least  one rating that is not zero.
        
            output = sae(input)                             # output of predicted ratings
            
            target.required_grad = False                    # when we apply stochastic gradient descent, we want to make sure the gradient is computed only with respect to the input and not the target.
                                                            # 'target.required_grad = False' will make sure that we don't compute the gradient with respect to the target and that will save a lot of computations and that optimizes the code.
                                                            
            output[target == 0] = 0                         # We wanna take the values of our output vector that is the predicted ratings in our output vector,
                                                            # such as the corresponding ratings in the target vector are equal to zero. We're just taking the same indexes of the ratings
                                                            # that were equal to zero in the input vector. And for these indexes of the output vector, we will set the values corresponding
                                                            # to these indexes to zero. These zero values will not count in the computations of the error, 
                                                            # and so they won't have impact on the updates of the different weights right after having measured the error.
                                                            # After we've measured the error, the weights are updated by the RMSprop Optimizer and updating these weights require
                                                            # some computations and in these computations, these values here don't count.
                                                            # Even if they're not equal to zero, they don't count, and so, to save up some memory, again, we set them to zero.
           
            loss = criterion(output, target)                # First argument, we're gonna input our vector of predicted ratings, which is output,
                                                            # and second argument is the vector of real ratings, that is the truth, which is our target.
                                                            
            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) # mean_corrector is equal to the number of movies over the number of movies that have positive ratings.
                                                            # a small 1e-10 is added that will make sure the denominator is different than zero.
                                                            # Why mean_corrector?
                                                            # This actually represents the average of the error, but by only considering the movies that were rated,
                                                            # the movies that at least got one to five ratings.And that, we need to do this because we only considered here
                                                            # the movies that have non-zero ratings.Since then we will compute some mean,this mean has to be computed
                                                            # only on the movies that we consider.That is, the movies that got non-zero ratings.This mean correcter variable is just
                                                            # to adapt to this consideration of the movies that got non-zero ratings.We need to do this because this will then be
                                                            # mathematically relevant to compute the mean of the errors.
                                                            
            loss.backward()                                 # In this next step, we're gonna call the backward method for the loss. This call of the backward method will just tell
                                                            # in which direction we need to update the different weights. Do we need to increase the weight or decrease the weight?
                                                            # That's what the backward method will make sure of doing properly.  
                                                            
            train_loss += np.sqrt(loss.data.item()*mean_corrector)
            """We're gonna update it, train loss and I'm going to update it this way. I'm gonna add a +, then an =, and that means that we're incrementing it by something.
               We're adding something to the original value of train loss, which, so far, is zero. So, basically what I'm adding here will be the new value of train loss.
               And then at the next round, that is for the next user, the new value of train loss will be the old value of train lossplus what we're gonna add here.
               So, what are we going to add here? We're gonna take the loss that is generated after the prediction was done. That is basically the difference between
               the real rating and the predicted rating. And so to get this, we need to take our loss object, then add a dot and now we're gonna get
               the part of this loss object that contains the error. We excess to the data in the loss object and then we need to take the index of the data
               that contains this train loss, and this index is actually zero. That's when we're gonna use our mean corrector, because in order to get this adjusted mean over the movies
               that got non-zero ratings by the user, we need to multiply our loss here, by this adjustment factor which is our mean corrector.
               So, we're just adjusting this loss with this mean corrector factor to compute this relevant mean, and then since this loss data is the squared error
               and we want to get the state of the art error, that is the one degree loss, we will take the root of this loss.data.mean_corrector.
               That is this adjusted square loss. And so we're gonna use here NumPy. It's been a while, but we still need NumPy and that's because we wanna take
               the square root function, which is S-Q-R-T. Remember, we already used it, so we're gonna put that in parentheses. And that is just to get the state of the art error,
               including that adjustment here."""
             
            s += 1.
            optimizer.step()
            """backward()-> indicates direction where as optimzer indicates->intensity"""
    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))

"""## Testing the SAE"""
test_loss = 0 # initialize loss error
s = 0. # float number,  number of users who atleast rated one movie, this variable is required to optimize the memory by not doing any computation for the movies not rated by user.
for id_user in range(nb_users):
    input = Variable(training_set[id_user])                                                           
    target = Variable(test_set[id_user])
    if torch.sum(target.data > 0) > 0:
        output = sae(input)
        target.required_grad = False
        output[target == 0] = 0
        loss = criterion(output, target)                
        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)                              
        test_loss += np.sqrt(loss.data.item()*mean_corrector)             
        s += 1.
print('Test loss: '+str(test_loss/s))



